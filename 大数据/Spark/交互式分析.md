

# Spark交互式分析

从 README 文本文件中创建一个新的 RDD

~~~
scala> val textFile=sc.textFile("README.md")
textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile
at <console>:27
scala> textFile.count()
res0: Long = 95

~~~

使用 filter 在这个文件里返回一个包含子数据集的新 RDD

~~~
scala> val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <c
onsole>:29
scala> textFile.filter(line => line.contains("Spark")).count()
res2: Long = 17

~~~

找到一行中最多的单词数量

~~~
scala> textFile.map(line => line.split("").size).reduce((a,b) => if (a > b) a else b)
res3: Int = 121

~~~

使用 Math.max() 函数让代码更容易理解

~~~
scala> import java.lang.Math
import java.lang.Math
scala> textFile.map(line => line.split(" ").size).reduce((a, b) => Math.max(a, b))
res5: Int = 15

~~~

Spark 能很容易地实现 MapReduce

~~~
scala> val wordCounts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1
)).reduceByKey((a, b) => a + b)
wordCounts: spark.RDD[(String, Int)] = spark.ShuffledAggregatedRDD@71f027b8

~~~

我们结合 flatMap, map 和 reduceByKey 来计算文件里每个单词出现的数量，它的结果是包含一组(String, Int) 键值对的 RDD。我们可以使用 [collect] 操作在我们的 shell 中收集单词的数量

~~~
scala> wordCounts.collect()
res6: Array[(String, Int)] = Array((means,1), (under,2), (this,3), (Because,1), (Pytho
n,2), (agree,1), (cluster.,1), ...)

~~~

缓存

~~~
scala> linesWithSpark.cache()
res7: spark.RDD[String] = spark.FilteredRDD@17e51082
scala> linesWithSpark.count()
res8: Long = 15
scala> linesWithSpark.count()
res9: Long = 15

~~~

